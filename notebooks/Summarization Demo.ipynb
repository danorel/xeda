{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danorel/Workspace/Education/University/NYU/Research/xeda\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Installation of required packages, defining API keys\n",
    "\n",
    "!pip install --quiet openai python-dotenv boto3 langchain chromadb\n",
    "\n",
    "import boto3\n",
    "import chromadb\n",
    "import copy\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import openai\n",
    "import typing as t\n",
    "import pandas as pd\n",
    "import typing as t\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "from boto3.session import Session\n",
    "from botocore.config import Config\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typings.pipeline import (\n",
    "    Pipeline,\n",
    "    PipelineEda4Sum,\n",
    "    PipelineItemEda4Sum,\n",
    "    AnnotatedPipelineEda4Sum,\n",
    "    AnnotatedPipelineItemEda4Sum,\n",
    "    AnnotatedPartialPipelineEda4Sum\n",
    ")\n",
    "from typings.annotation import PartialAnnotation\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv('.env'))\n",
    "\n",
    "from constants import (\n",
    "    AWS_ACCESS_KEY_ID,\n",
    "    AWS_SECRET_ACCESS_KEY,\n",
    "    AWS_S3_REGION_NAME,\n",
    "    AWS_S3_USE_SSL,\n",
    "    AWS_S3_BUCKET_NAME,\n",
    "    AWS_S3_ENDPOINT_URL,\n",
    "    GROUPS_CSV_PATH,\n",
    "    OPENAI_API_KEY,\n",
    "    VECTOR_STORE_COLLECTION,\n",
    "    VECTOR_STORE_HOST,\n",
    "    VECTOR_STORE_PORT\n",
    ")\n",
    "\n",
    "pretrained_embeddings = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "vector_store = chromadb.HttpClient(\n",
    "    host=VECTOR_STORE_HOST, \n",
    "    port=VECTOR_STORE_PORT\n",
    ")\n",
    "\n",
    "try:\n",
    "    vector_collection = vector_store.create_collection(\n",
    "        name=VECTOR_STORE_COLLECTION, \n",
    "        embedding_function=pretrained_embeddings,\n",
    "        metadata={\n",
    "            \"hnsw:space\": \"cosine\"\n",
    "        }\n",
    "    )\n",
    "except:\n",
    "    vector_collection = vector_store.get_collection(VECTOR_STORE_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@Guidance utils: pipeline annotator\n",
    "\n",
    "def find_item_set(\n",
    "    members: pd.DataFrame, pipeline_body_item: PipelineItemEda4Sum\n",
    ") -> t.Set[str]:\n",
    "    input_set_id = pipeline_body_item[\"inputSet\"][\"id\"]\n",
    "    members = members.loc[members[\"id\"] == input_set_id][\"members\"].iloc[0]\n",
    "    input_set = set(members[1:-1].split(\", \"))\n",
    "    return input_set\n",
    "\n",
    "def _find_delta_uniformity(\n",
    "    pipeline_item_current: PipelineItemEda4Sum, pipeline_item_next: PipelineItemEda4Sum\n",
    ") -> float:\n",
    "    return pipeline_item_next[\"uniformity\"] - pipeline_item_current[\"uniformity\"]\n",
    "\n",
    "def _find_delta_novelty(\n",
    "    pipeline_item_current: PipelineItemEda4Sum, pipeline_item_next: PipelineItemEda4Sum\n",
    ") -> float:\n",
    "    return pipeline_item_next[\"novelty\"] - pipeline_item_current[\"novelty\"]\n",
    "\n",
    "def _find_delta_diversity(\n",
    "    pipeline_item_current: PipelineEda4Sum, pipeline_item_next: PipelineEda4Sum\n",
    ") -> float:\n",
    "    return pipeline_item_next[\"distance\"] - pipeline_item_current[\"distance\"]\n",
    "\n",
    "def _find_utility_weights(\n",
    "    pipeline_item_current: PipelineEda4Sum, pipeline_item_next: PipelineEda4Sum\n",
    ") -> t.List[float]:\n",
    "    return [\n",
    "        pipeline_item_next[\"utilityWeights\"][i]\n",
    "        - pipeline_item_current[\"utilityWeights\"][i]\n",
    "        for i in range(3)\n",
    "    ]\n",
    "\n",
    "def _find_familiarity_curiosity(seen_galaxies, item_members) -> t.Tuple[float, float]:\n",
    "    if len(seen_galaxies) == 0:\n",
    "        return [0.0, 0.0]\n",
    "    else:\n",
    "        common_members_number = sum(1 for elem in item_members if elem in seen_galaxies)\n",
    "        familiarity = common_members_number / (len(seen_galaxies))\n",
    "        separate_members_number = sum(\n",
    "            1 for elem in item_members if elem not in seen_galaxies\n",
    "        )\n",
    "        curiosity = separate_members_number / (len(seen_galaxies))\n",
    "        return [familiarity, curiosity]\n",
    "\n",
    "def annotate_partial_pipeline(\n",
    "    groups_df: pd.DataFrame, partial_pipeline: PipelineEda4Sum\n",
    ") -> AnnotatedPartialPipelineEda4Sum:\n",
    "    seen_galaxies = []\n",
    "\n",
    "    length = len(partial_pipeline)\n",
    "    annotated_partial_pipeline: AnnotatedPartialPipelineEda4Sum = []\n",
    "\n",
    "    for item in range(length):\n",
    "        if item is not length - 1:\n",
    "            delta_uniformity = _find_delta_uniformity(\n",
    "                partial_pipeline[item], partial_pipeline[item + 1]\n",
    "            )\n",
    "            delta_novelty = _find_delta_novelty(partial_pipeline[item], partial_pipeline[item + 1])\n",
    "            delta_diversity = _find_delta_diversity(partial_pipeline[item], partial_pipeline[item + 1])\n",
    "            delta_utility_weights = _find_utility_weights(\n",
    "                partial_pipeline[item], partial_pipeline[item + 1]\n",
    "            )\n",
    "        else:\n",
    "            delta_uniformity = 0\n",
    "            delta_novelty = 0\n",
    "            delta_diversity = 0\n",
    "            delta_utility_weights = [0.0, 0.0, 0.0]\n",
    "\n",
    "        familiarity = 0.0\n",
    "        curiosity = 0.0\n",
    "\n",
    "        if \"requestData\" in partial_pipeline[item].keys():\n",
    "            input_set_id = partial_pipeline[item][\"selectedSetId\"]\n",
    "            item_members = groups_df.loc[groups_df[\"id\"] == input_set_id][\"members\"]\n",
    "\n",
    "            for i in item_members:\n",
    "                list_members = i[1:-1].split(\", \")\n",
    "                result_members = [int(num) for num in list_members]\n",
    "\n",
    "            if item_members.empty:\n",
    "                print(f\"Node[id={input_set_id}] is missing in .csv\")\n",
    "            else:\n",
    "                familiarity, curiosity = _find_familiarity_curiosity(\n",
    "                    seen_galaxies, result_members\n",
    "                )\n",
    "                seen_galaxies.extend(result_members)\n",
    "\n",
    "        partial_annotation = PartialAnnotation(\n",
    "            current_operator=partial_pipeline[item][\"operator\"],\n",
    "            current_dimension=partial_pipeline[item][\"checkedDimension\"],\n",
    "            delta_uniformity=delta_uniformity,\n",
    "            delta_novelty=delta_novelty,\n",
    "            delta_diversity=delta_diversity,\n",
    "            delta_utilityWeights=delta_utility_weights,\n",
    "            current_uniformity=partial_pipeline[item][\"uniformity\"],\n",
    "            current_novelty=partial_pipeline[item][\"novelty\"],\n",
    "            current_diversity=partial_pipeline[item][\"distance\"],\n",
    "            current_utilityWeights=partial_pipeline[item][\"utilityWeights\"],\n",
    "            familiarity=familiarity,\n",
    "            curiosity=curiosity,\n",
    "        )\n",
    "        annotated_pipeline_item = AnnotatedPipelineItemEda4Sum(\n",
    "            **partial_pipeline[item], annotation=partial_annotation\n",
    "        )\n",
    "        annotated_partial_pipeline.append(annotated_pipeline_item)\n",
    "\n",
    "    return annotated_partial_pipeline\n",
    "\n",
    "def node_to_encoding(node):\n",
    "    annotation = node[\"annotation\"]\n",
    "    node_encoding = []\n",
    "    for k, v in annotation.items():\n",
    "        if isinstance(v, dict):\n",
    "            for key in v:\n",
    "                node_encoding.append(f\"{k}_{key} = {v[key]}\")\n",
    "        else:\n",
    "            node_encoding.append(f\"{k} = {v}\")\n",
    "    return ', '.join(node_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@Guidance utils: summarization model and inference\n",
    "\n",
    "summarization_prompt_template = \"\"\"Write a concise summary of \"{text}\". CONCISE SUMMARY:\"\"\"\n",
    "summarization_prompt = PromptTemplate.from_template(summarization_prompt_template)\n",
    "\n",
    "summarization_llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "summarization_chain = LLMChain(llm=summarization_llm, prompt=summarization_prompt)\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=summarization_chain, document_variable_name=\"text\")\n",
    "\n",
    "groups_df = pd.read_csv(GROUPS_CSV_PATH)\n",
    "\n",
    "def guide(partial_pipeline: Pipeline):\n",
    "    partial_annotated_pipeline = annotate_partial_pipeline(groups_df, partial_pipeline)\n",
    "    partial_pipeline_partial_annotation = ';'.join([node_to_encoding(node) for node in partial_annotated_pipeline])\n",
    "    partial_annotation_embeddings = pretrained_embeddings([partial_pipeline_partial_annotation])\n",
    "    \n",
    "    most_similar_responses = vector_collection.query(\n",
    "        query_embeddings=partial_annotation_embeddings,\n",
    "        n_results=3,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    if not len(most_similar_responses['documents'][0]):\n",
    "        return \"Not found any similar pipelines\"\n",
    "    else:\n",
    "        return stuff_chain.run(make_natural_language_documents(most_similar_responses['documents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@Guidance natural language utils \n",
    "\n",
    "def make_instruction(name, value):\n",
    "    return f\"{name} = {value}\" if value else None\n",
    "\n",
    "def make_natural_language_documents(docs: list):\n",
    "    for doc in docs:\n",
    "        pipeline = json.loads(doc[0])\n",
    "        last_node = pipeline[-1]\n",
    "        # Extract natural language properties\n",
    "        total_length, operator, checked_dimension, remaining_operators = (\n",
    "            last_node['annotation']['total_length'],\n",
    "            last_node['operator'],\n",
    "            last_node['checkedDimension'],\n",
    "            last_node['annotation']['remaining_operators']\n",
    "        )\n",
    "        # Derive natural language guidance features\n",
    "        remaining_operators_count = sum(v for v in remaining_operators.values())\n",
    "        remaining_operators_distribution = ', '.join([f\"{operator} = {operator_count / remaining_operators_count}%\" for operator, operator_count in remaining_operators.items()])\n",
    "        # Build natural language query for summarization\n",
    "        natural_language_instructions = [instruction for instruction in [\n",
    "            make_instruction(name=\"most_probable_pipeline_length\", value=total_length),\n",
    "            make_instruction(name=\"most_probable_operator\", value=operator),\n",
    "            make_instruction(name=\"reachable_attribute_by_operator\", value=checked_dimension),\n",
    "            make_instruction(name=\"operator_probability_distribution\", value=remaining_operators_distribution),\n",
    "        ] if instruction is not None]\n",
    "        natural_language_document = Document(page_content=\", \".join(natural_language_instructions))\n",
    "        yield natural_language_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"notebooks/data/partial-pipeline.json\", \"r\") as f:\n",
    "    partial_pipeline = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most probable pipeline length is 4, the most probable operator is \"by_neighbors\", and the reachable attribute by the operator is \"g\".'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guide(partial_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xeda_3.8.15",
   "language": "python",
   "name": "xeda_3.8.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
